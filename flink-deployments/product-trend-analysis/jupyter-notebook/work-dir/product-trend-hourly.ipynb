{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "471f257e-a83f-4cf0-b734-8fa9a9189f1e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting plotly\n",
      "  Downloading plotly-5.24.1-py3-none-any.whl (19.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.1/19.1 MB\u001b[0m \u001b[31m41.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from plotly) (23.1)\n",
      "Collecting tenacity>=6.2.0\n",
      "  Downloading tenacity-9.0.0-py3-none-any.whl (28 kB)\n",
      "Installing collected packages: tenacity, plotly\n",
      "Successfully installed plotly-5.24.1 tenacity-9.0.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#pip install plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "878768b6-7f52-400f-8837-5e338e9fd634",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.9.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.3/8.3 MB\u001b[0m \u001b[31m26.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting pillow>=8\n",
      "  Downloading pillow-11.0.0-cp39-cp39-manylinux_2_28_x86_64.whl (4.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m52.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting kiwisolver>=1.3.1\n",
      "  Downloading kiwisolver-1.4.7-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m64.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.9/dist-packages (from matplotlib) (2.8.2)\n",
      "Collecting fonttools>=4.22.0\n",
      "  Downloading fonttools-4.54.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m32.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.9/dist-packages (from matplotlib) (1.24.3)\n",
      "Collecting importlib-resources>=3.2.0\n",
      "  Downloading importlib_resources-6.4.5-py3-none-any.whl (36 kB)\n",
      "Collecting pyparsing>=2.3.1\n",
      "  Downloading pyparsing-3.2.0-py3-none-any.whl (106 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.9/106.9 kB\u001b[0m \u001b[31m37.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib) (23.1)\n",
      "Collecting contourpy>=1.0.1\n",
      "  Downloading contourpy-1.3.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (321 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m321.9/321.9 kB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting cycler>=0.10\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.9/dist-packages (from importlib-resources>=3.2.0->matplotlib) (3.15.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Installing collected packages: pyparsing, pillow, kiwisolver, importlib-resources, fonttools, cycler, contourpy, matplotlib\n",
      "\u001b[33m  WARNING: The scripts fonttools, pyftmerge, pyftsubset and ttx are installed in '/home/NBuser/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed contourpy-1.3.0 cycler-0.12.1 fonttools-4.54.1 importlib-resources-6.4.5 kiwisolver-1.4.7 matplotlib-3.9.2 pillow-11.0.0 pyparsing-3.2.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#pip install matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2914e13f-a85d-4def-b709-9b7bb2a14579",
   "metadata": {},
   "source": [
    "## Record Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "89521062-3a35-4686-ad43-55c34d9dc591",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 52:========================================================(40 + 0) / 40]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of records in Delta table 5036\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from delta import *\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "from pathlib import Path\n",
    "from pyspark.sql.functions import col\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.filterwarnings('ignore', category=UserWarning, message=\"Using an existing Spark session; only runtime SQL configurations will take effect.\")\n",
    "\n",
    "conf = SparkConf().setAppName('Low Inventory Alert Count')\\\n",
    "        .set(\"spark.jars.packages\", \"io.delta:delta-core_2.12:2.3.0\")\\\n",
    "        .set(\"spark.sql.catalog.spark_catalog\",\"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\\\n",
    "        .set(\"spark.sql.extensions\",\"io.delta.sql.DeltaSparkSessionExtension\")\\\n",
    "        .set(\"log4j.logger.org.apache.hadoop.util.NativeCodeLoader\", \"ERROR\")\\\n",
    "        .set(\"log4j.logger.org.apache.spark.internal.config.native-code-path\", \"ERROR\")\n",
    "\n",
    "#builder = spark.builder.appName(\"MyApp\") \\\n",
    "#    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "#    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "\n",
    "\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "#spark = configure_spark_with_delta_pip(builder).getOrCreate()\n",
    "\n",
    "delta_table_path = \"/opt/spark/delta-tables/product-views-hourly\"\n",
    "\n",
    "# Read data from the Delta table\n",
    "#df = spark.read.format(\"delta\").load(\"/opt/spark/delta-tables/low_stock_alert\")\n",
    "#print(f\"# of Low Stock alerts: {df.count()}\")\n",
    "\n",
    "if Path(delta_table_path, \"_delta_log\").exists():\n",
    "    df = spark.read.format(\"delta\").load(delta_table_path)\n",
    "    \n",
    "    print(f\"# of records in Delta table {df.count()}\")\n",
    "else:\n",
    "    print(\"No Data available currently\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd61378-9008-4dbe-8e7a-9a6067511a43",
   "metadata": {},
   "source": [
    "## View Records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae20336b-0c87-4fc4-884e-2d16b0439a90",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+----------+--------------------+--------------------+\n",
      "|          product_id|        product_name|               brand|view_count|           starttime|             endtime|\n",
      "+--------------------+--------------------+--------------------+----------+--------------------+--------------------+\n",
      "|57caf4a9-3c81-4e6...|Spectra5 TrailRun...|Okuneva, McCullou...|       167|2024-10-24 18:45:...|2024-10-24 19:45:...|\n",
      "|57caf4a9-3c81-4e6...|Spectra5 TrailRun...|Okuneva, McCullou...|       167|2024-10-24 19:00:...|2024-10-24 20:00:...|\n",
      "|57caf4a9-3c81-4e6...|Spectra5 TrailRun...|Okuneva, McCullou...|       167|2024-10-24 19:15:...|2024-10-24 20:15:...|\n",
      "|57caf4a9-3c81-4e6...|Spectra5 TrailRun...|Okuneva, McCullou...|       167|2024-10-24 19:30:...|2024-10-24 20:30:...|\n",
      "|9f56111d-da20-49d...|TrailRunner Sidek...|Okuneva, McCullou...|       194|2024-10-24 18:45:...|2024-10-24 19:45:...|\n",
      "|9f56111d-da20-49d...|TrailRunner Sidek...|Okuneva, McCullou...|       194|2024-10-24 19:00:...|2024-10-24 20:00:...|\n",
      "|9f56111d-da20-49d...|TrailRunner Sidek...|Okuneva, McCullou...|       194|2024-10-24 19:15:...|2024-10-24 20:15:...|\n",
      "|9f56111d-da20-49d...|TrailRunner Sidek...|Okuneva, McCullou...|       194|2024-10-24 19:30:...|2024-10-24 20:30:...|\n",
      "|d993a27e-3267-4e8...|Sidekick TrailRun...|Okuneva, McCullou...|       330|2024-10-24 18:45:...|2024-10-24 19:45:...|\n",
      "|d993a27e-3267-4e8...|Sidekick TrailRun...|Okuneva, McCullou...|       330|2024-10-24 19:00:...|2024-10-24 20:00:...|\n",
      "|d993a27e-3267-4e8...|Sidekick TrailRun...|Okuneva, McCullou...|       330|2024-10-24 19:15:...|2024-10-24 20:15:...|\n",
      "|d993a27e-3267-4e8...|Sidekick TrailRun...|Okuneva, McCullou...|       330|2024-10-24 19:30:...|2024-10-24 20:30:...|\n",
      "|195fbd5c-56ea-452...|Sidekick TrailRun...|Okuneva, McCullou...|       196|2024-10-24 18:45:...|2024-10-24 19:45:...|\n",
      "|195fbd5c-56ea-452...|Sidekick TrailRun...|Okuneva, McCullou...|       196|2024-10-24 19:00:...|2024-10-24 20:00:...|\n",
      "|195fbd5c-56ea-452...|Sidekick TrailRun...|Okuneva, McCullou...|       196|2024-10-24 19:15:...|2024-10-24 20:15:...|\n",
      "|195fbd5c-56ea-452...|Sidekick TrailRun...|Okuneva, McCullou...|       196|2024-10-24 19:30:...|2024-10-24 20:30:...|\n",
      "|57caf4a9-3c81-4e6...|Spectra5 TrailRun...|Okuneva, McCullou...|       351|2024-10-31 11:45:...|2024-10-31 12:45:...|\n",
      "|57caf4a9-3c81-4e6...|Spectra5 TrailRun...|Okuneva, McCullou...|       351|2024-10-31 12:00:...|2024-10-31 13:00:...|\n",
      "|57caf4a9-3c81-4e6...|Spectra5 TrailRun...|Okuneva, McCullou...|       351|2024-10-31 12:15:...|2024-10-31 13:15:...|\n",
      "|57caf4a9-3c81-4e6...|Spectra5 TrailRun...|Okuneva, McCullou...|       351|2024-10-31 12:30:...|2024-10-31 13:30:...|\n",
      "+--------------------+--------------------+--------------------+----------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "# of records in Delta table None\n"
     ]
    }
   ],
   "source": [
    "from delta import *\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "from pathlib import Path\n",
    "from pyspark.sql.functions import col\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.filterwarnings('ignore', category=UserWarning, message=\"Using an existing Spark session; only runtime SQL configurations will take effect.\")\n",
    "\n",
    "conf = SparkConf().setAppName('Low Inventory Alert Count')\\\n",
    "        .set(\"spark.jars.packages\", \"io.delta:delta-core_2.12:2.3.0\")\\\n",
    "        .set(\"spark.sql.catalog.spark_catalog\",\"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\\\n",
    "        .set(\"spark.sql.extensions\",\"io.delta.sql.DeltaSparkSessionExtension\")\\\n",
    "        .set(\"log4j.logger.org.apache.hadoop.util.NativeCodeLoader\", \"ERROR\")\\\n",
    "        .set(\"log4j.logger.org.apache.spark.internal.config.native-code-path\", \"ERROR\")\n",
    "\n",
    "#builder = spark.builder.appName(\"MyApp\") \\\n",
    "#    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "#    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "\n",
    "\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "#spark = configure_spark_with_delta_pip(builder).getOrCreate()\n",
    "\n",
    "delta_table_path = \"/opt/spark/delta-tables/product-views-hourly\"\n",
    "\n",
    "# Read data from the Delta table\n",
    "#df = spark.read.format(\"delta\").load(\"/opt/spark/delta-tables/low_stock_alert\")\n",
    "#print(f\"# of Low Stock alerts: {df.count()}\")\n",
    "\n",
    "if Path(delta_table_path, \"_delta_log\").exists():\n",
    "    df = spark.read.format(\"delta\").load(delta_table_path)\n",
    "    # Convert total_sale_price to a standard numeric format\n",
    "    #df = df.withColumn(\"total_sale_pice\", col(\"total_sale_pice\").cast(\"decimal(20, 8)\"))\n",
    "\n",
    "    print(f\"# of records in Delta table {df.show()}\")\n",
    "else:\n",
    "    print(\"No Data available currently\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3591ac-d170-4678-941a-0fc8f0e21f2b",
   "metadata": {},
   "source": [
    "## Visuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26a6bee9-4698-4591-9c27-8ec3f3ab9ea8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<iframe\n",
       "    scrolling=\"no\"\n",
       "    width=\"100%\"\n",
       "    height=\"545px\"\n",
       "    src=\"iframe_figures/figure_6.html\"\n",
       "    frameborder=\"0\"\n",
       "    allowfullscreen\n",
       "></iframe>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<iframe\n",
       "    scrolling=\"no\"\n",
       "    width=\"100%\"\n",
       "    height=\"545px\"\n",
       "    src=\"iframe_figures/figure_6.html\"\n",
       "    frameborder=\"0\"\n",
       "    allowfullscreen\n",
       "></iframe>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<iframe\n",
       "    scrolling=\"no\"\n",
       "    width=\"100%\"\n",
       "    height=\"545px\"\n",
       "    src=\"iframe_figures/figure_6.html\"\n",
       "    frameborder=\"0\"\n",
       "    allowfullscreen\n",
       "></iframe>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<iframe\n",
       "    scrolling=\"no\"\n",
       "    width=\"100%\"\n",
       "    height=\"545px\"\n",
       "    src=\"iframe_figures/figure_6.html\"\n",
       "    frameborder=\"0\"\n",
       "    allowfullscreen\n",
       "></iframe>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<iframe\n",
       "    scrolling=\"no\"\n",
       "    width=\"100%\"\n",
       "    height=\"545px\"\n",
       "    src=\"iframe_figures/figure_6.html\"\n",
       "    frameborder=\"0\"\n",
       "    allowfullscreen\n",
       "></iframe>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<iframe\n",
       "    scrolling=\"no\"\n",
       "    width=\"100%\"\n",
       "    height=\"545px\"\n",
       "    src=\"iframe_figures/figure_6.html\"\n",
       "    frameborder=\"0\"\n",
       "    allowfullscreen\n",
       "></iframe>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<iframe\n",
       "    scrolling=\"no\"\n",
       "    width=\"100%\"\n",
       "    height=\"545px\"\n",
       "    src=\"iframe_figures/figure_6.html\"\n",
       "    frameborder=\"0\"\n",
       "    allowfullscreen\n",
       "></iframe>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<iframe\n",
       "    scrolling=\"no\"\n",
       "    width=\"100%\"\n",
       "    height=\"545px\"\n",
       "    src=\"iframe_figures/figure_6.html\"\n",
       "    frameborder=\"0\"\n",
       "    allowfullscreen\n",
       "></iframe>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#from delta import *\n",
    "#from pyspark.sql import SparkSession\n",
    "#from pyspark import SparkConf\n",
    "#from pathlib import Path\n",
    "#from pyspark.sql.functions import col\n",
    "#import pandas as pd\n",
    "#import plotly.express as px\n",
    "#from IPython.display import display\n",
    "#import warnings\n",
    "#import plotly.io as pio\n",
    "#pio.renderers.default = \"iframe\"\n",
    "#warnings.filterwarnings('ignore')\n",
    "#warnings.filterwarnings('ignore', category=UserWarning, message=\"Using an existing Spark session; only runtime SQL configurations will take effect.\")\n",
    "\n",
    "#conf = SparkConf().setAppName('Low Inventory Alert Count')\\\n",
    "#        .set(\"spark.jars.packages\", \"io.delta:delta-core_2.12:2.3.0\")\\\n",
    "#        .set(\"spark.sql.catalog.spark_catalog\",\"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\\\n",
    "#        .set(\"spark.sql.extensions\",\"io.delta.sql.DeltaSparkSessionExtension\")\\\n",
    "#        .set(\"log4j.logger.org.apache.hadoop.util.NativeCodeLoader\", \"ERROR\")\\\n",
    "#        .set(\"log4j.logger.org.apache.spark.internal.config.native-code-path\", \"ERROR\")\n",
    "\n",
    "#builder = spark.builder.appName(\"MyApp\") \\\n",
    "#    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "#    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "\n",
    "\n",
    "#spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "\n",
    "#spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "#spark = configure_spark_with_delta_pip(builder).getOrCreate()\n",
    "\n",
    "#delta_table_path = \"/opt/spark/delta-tables/product-views-hourly\"\n",
    "\n",
    "# Read data from the Delta table\n",
    "#df = spark.read.format(\"delta\").load(\"/opt/spark/delta-tables/low_stock_alert\")\n",
    "#print(f\"# of Low Stock alerts: {df.count()}\")\n",
    "\n",
    "#if Path(delta_table_path, \"_delta_log\").exists():\n",
    "#    df = spark.read.format(\"delta\").load(delta_table_path)\n",
    "#    # Convert total_sale_price to a standard numeric format\n",
    "#    #df = df.withColumn(\"total_sale_pice\", col(\"total_sale_pice\").cast(\"decimal(20, 8)\"))\n",
    "#    latest_df_pd = df.toPandas()\n",
    "    \n",
    "    # Convert starttime to datetime\n",
    "#    latest_df_pd['starttime'] = pd.to_datetime(latest_df_pd['starttime'])\n",
    "#    latest_df_pd['endtime'] = pd.to_datetime(latest_df_pd['endtime'])\n",
    "    \n",
    "    # Group by starttime, endtime, product_id, and product_name; get the maximum view_count for each group\n",
    "#    grouped_df = (\n",
    "#        latest_df_pd.groupby(['starttime', 'endtime', 'product_id', 'product_name'], as_index=False)\n",
    "#        .agg({'view_count': 'max'})\n",
    "#    )\n",
    "    \n",
    "    # Sort within each time window by view_count in descending order, then pick the top 3 for each (starttime, endtime)\n",
    "#    top_3_df = (\n",
    "#        grouped_df.sort_values(by=['starttime', 'endtime', 'view_count'], ascending=[True, True, False])\n",
    "#        .groupby(['starttime', 'endtime'])\n",
    "#        .head(3)\n",
    "#        .reset_index(drop=True)\n",
    "#    )\n",
    "\n",
    "    # Display the result\n",
    "    #print(top_3_df)\n",
    "    \n",
    "    # Create individual DataFrames for each starttime and endtime\n",
    "#    unique_time_windows = top_3_df[['starttime', 'endtime']].drop_duplicates()\n",
    "    \n",
    "#    for index, row in unique_time_windows.iterrows():\n",
    "#        starttime = row['starttime']\n",
    "#        endtime = row['endtime']\n",
    "        \n",
    "        # Filter the top_3_df for this specific time window\n",
    "#        window_df = top_3_df[(top_3_df['starttime'] == starttime) & (top_3_df['endtime'] == endtime)]\n",
    "        \n",
    "        # Create a bar plot for this time window\n",
    "#        fig = px.bar(\n",
    "#            window_df,\n",
    "#            x='product_name',\n",
    "#            y='view_count',\n",
    "#            color='product_name',\n",
    "#            title=f'Top Trending Products from {starttime} to {endtime}',\n",
    "#            labels={'view_count': 'Count', 'product_name': 'Product Name'}\n",
    "#        )\n",
    "        \n",
    "        # Show the chart\n",
    "        #fig.show()\n",
    "#        display(fig)\n",
    "\n",
    "#else:\n",
    "#    print(\"No Data available currently\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2606378f-448e-4ac0-ae84-b0c2529c7eb7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<iframe\n",
       "    scrolling=\"no\"\n",
       "    width=\"100%\"\n",
       "    height=\"545px\"\n",
       "    src=\"iframe_figures/figure_2.html\"\n",
       "    frameborder=\"0\"\n",
       "    allowfullscreen\n",
       "></iframe>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from delta import *\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "from pathlib import Path\n",
    "from pyspark.sql.functions import col\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "from IPython.display import display\n",
    "import warnings\n",
    "import plotly.io as pio\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "pio.renderers.default = \"iframe\"\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.filterwarnings('ignore', category=UserWarning, message=\"Using an existing Spark session; only runtime SQL configurations will take effect.\")\n",
    "\n",
    "conf = SparkConf().setAppName('Low Inventory Alert Count')\\\n",
    "        .set(\"spark.jars.packages\", \"io.delta:delta-core_2.12:2.3.0\")\\\n",
    "        .set(\"spark.sql.catalog.spark_catalog\",\"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\\\n",
    "        .set(\"spark.sql.extensions\",\"io.delta.sql.DeltaSparkSessionExtension\")\\\n",
    "        .set(\"log4j.logger.org.apache.hadoop.util.NativeCodeLoader\", \"ERROR\")\\\n",
    "        .set(\"log4j.logger.org.apache.spark.internal.config.native-code-path\", \"ERROR\")\n",
    "\n",
    "#builder = spark.builder.appName(\"MyApp\") \\\n",
    "#    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "#    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "\n",
    "\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "#spark = configure_spark_with_delta_pip(builder).getOrCreate()\n",
    "\n",
    "delta_table_path = \"/opt/spark/delta-tables/product-views-hourly\"\n",
    "\n",
    "# Read data from the Delta table\n",
    "#df = spark.read.format(\"delta\").load(\"/opt/spark/delta-tables/low_stock_alert\")\n",
    "#print(f\"# of Low Stock alerts: {df.count()}\")\n",
    "\n",
    "if Path(delta_table_path, \"_delta_log\").exists():\n",
    "    df = spark.read.format(\"delta\").load(delta_table_path)\n",
    "    # Convert total_sale_price to a standard numeric format\n",
    "    #df = df.withColumn(\"total_sale_pice\", col(\"total_sale_pice\").cast(\"decimal(20, 8)\"))\n",
    "    latest_df_pd = df.toPandas()\n",
    "    \n",
    "    # Convert starttime to datetime\n",
    "    latest_df_pd['starttime'] = pd.to_datetime(latest_df_pd['starttime'])\n",
    "    latest_df_pd['endtime'] = pd.to_datetime(latest_df_pd['endtime'])\n",
    "    \n",
    "    # Group by starttime, endtime, product_id, and product_name; get the maximum view_count for each group\n",
    "    grouped_df = (\n",
    "        latest_df_pd.groupby(['starttime', 'endtime', 'product_id', 'product_name'], as_index=False)\n",
    "        .agg({'view_count': 'max'})\n",
    "    )\n",
    "    \n",
    "    # Sort within each time window by view_count in descending order, then pick the top 3 for each (starttime, endtime)\n",
    "    top_3_df = (\n",
    "        grouped_df.sort_values(by=['starttime', 'endtime', 'view_count'], ascending=[True, True, False])\n",
    "        .groupby(['starttime', 'endtime'])\n",
    "        .head(3)\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    # Display the result\n",
    "    #print(top_3_df)\n",
    "    \n",
    "    # Create a figure\n",
    "    fig = go.Figure()\n",
    "    unique_time_windows = top_3_df[['starttime', 'endtime']].drop_duplicates()\n",
    "    # Create a dropdown list for time windows\n",
    "    time_window_options = [\n",
    "        {'label': f\"{row['starttime']} to {row['endtime']}\", 'value': index}\n",
    "        for index, row in unique_time_windows.iterrows()\n",
    "    ]\n",
    "\n",
    "    # Initial data for the first time window\n",
    "    initial_window = unique_time_windows.iloc[0]\n",
    "    initial_starttime = initial_window['starttime']\n",
    "    initial_endtime = initial_window['endtime']\n",
    "    initial_window_df = top_3_df[(top_3_df['starttime'] == initial_starttime) & (top_3_df['endtime'] == initial_endtime)]\n",
    "\n",
    "    # Add the initial data to the figure\n",
    "    fig.add_trace(go.Bar(\n",
    "        x=initial_window_df['product_name'],\n",
    "        y=initial_window_df['view_count'],\n",
    "        name=f'Top Trending Products from {initial_starttime} to {initial_endtime}',\n",
    "    ))\n",
    "\n",
    "    # Update layout with dropdown menu\n",
    "    fig.update_layout(\n",
    "        title='Top Trending Products',\n",
    "        xaxis_title='Product Name',\n",
    "        yaxis_title='View Count',\n",
    "        updatemenus=[\n",
    "            {\n",
    "                'buttons': [\n",
    "                    {\n",
    "                        'label': f\"{row['starttime']} to {row['endtime']}\",\n",
    "                        'method': 'update',\n",
    "                        'args': [\n",
    "                            {'x': [top_3_df[(top_3_df['starttime'] == row['starttime']) & (top_3_df['endtime'] == row['endtime'])]\n",
    "                                     ['product_name']],\n",
    "                             'y': [top_3_df[(top_3_df['starttime'] == row['starttime']) & (top_3_df['endtime'] == row['endtime'])]\n",
    "                                     ['view_count']],\n",
    "                             'name': f'Top Trending Products from {row[\"starttime\"]} to {row[\"endtime\"]}'}\n",
    "                        ]\n",
    "                    }\n",
    "                    for index, row in unique_time_windows.iterrows()\n",
    "                ],\n",
    "                'direction': 'down',\n",
    "                'showactive': True,\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Show the figure\n",
    "    fig.show()\n",
    "\n",
    "else:\n",
    "    print(\"No Data available currently\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc8002e-8a39-48e5-9107-13204a2cb3e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
